{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lime\n",
    "\n",
    "LIME (Local Interpretable Model-agnostic Explanations) is a local explanation method introduced by Ribeiro et al. \\[LIME\\]. More precisely, LIME is a Feature attribution method which means that the method computes for each feature of an input sample its importance in the prediction. To do so, LIME uses perturbations of the considered sample and their corresponding (and perturbated) predictions to identify features of importance. One of the advantages of such a black-box method is that it does not need to have access to the inner working of the model but only to its input(s) and output(s).\n",
    "\n",
    "The key idea of LIME is to *mask* some features and to consider these features as of importance if such perturbation(s) also strongly modifies the prediction. To be exhaustive, the method should consider each and every combination of features which not achievable in practice. To alleviate this issue, the authors proposed to first segment the image into groups of features (in case of an image sample, into superpixels) and then to use these groups of features instead of a individual features when computing the masks. Even if it drastically reduces the number of possible combinations, the number of combinations of groups of features is still too large to be considered. A surrogate model is trained to predict the perturbation (in term of prediction of the initial model) from a vector representation of the perturbated sample. The surrogate model is finally used to determine the combination of groups of features that deteriorate the most the prediction of the initial model. \n",
    "\n",
    "In the next, you will find a source code for loading a model (pre-trained on Imagenet), an image (from Imagenet) and to execute LIME. The main objective of this session is to manipulate the various parameters of the method and try to identify a good parameter setting.\n",
    "\n",
    "\\[LIME\\] Ribeiro, M. T., Singh, S., & Guestrin, C. (2016, August). \" Why should i trust you?\" Explaining the predictions of any classifier. In Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining (pp. 1135-1144)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import a pre-trained model, explain with Lime\n",
    "\n",
    "The following cells provide source code for loading a model, here Xception, pre-trained on Imagenet dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import a pre-trained Xception model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "model_builder = tf.keras.applications.xception.Xception\n",
    "preprocess_input = tf.keras.applications.xception.preprocess_input\n",
    "decode_predictions = tf.keras.applications.xception.decode_predictions\n",
    "model = model_builder(weights=\"imagenet\", classifier_activation=\"softmax\")\n",
    "\n",
    "# expected input size for Xception\n",
    "img_size = (299, 299)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load an image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def get_img_array(img_path, size):\n",
    "    # `img` is a PIL image of size 299x299\n",
    "    img = tf.keras.preprocessing.image.load_img(img_path, target_size=size)\n",
    "    # `array` is a float32 Numpy array of shape (299, 299, 3)\n",
    "    array = tf.keras.preprocessing.image.img_to_array(img)\n",
    "    # We add a dimension to transform our array into a \"batch\"\n",
    "    # of size (1, 299, 299, 3)\n",
    "    array = np.expand_dims(array, axis=0)\n",
    "    return preprocess_input(array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def make_prediction(img_array):\n",
    "    preds = model.predict(img_array).flatten()\n",
    "    pred_index = np.argmax(preds)  # we will explain for this specific class\n",
    "    labels = decode_predictions(np.asarray([preds]), top=3)[0]\n",
    "    labels = [[label[1], label[2]] for label in labels]    \n",
    "    \n",
    "    return preds, pred_index, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lime Explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create a Lime explanation** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate saliency with LIME algorithm\n",
    "\n",
    "from lime import lime_image\n",
    "\n",
    "def get_lime_explanation(img_array, pred_index, top_labels, hide_color, num_lime_features, num_samples):\n",
    "    explainer = lime_image.LimeImageExplainer(random_state=0) # for reproductibility\n",
    "\n",
    "    explanation = explainer.explain_instance(\n",
    "        img_array,\n",
    "        model.predict,\n",
    "        top_labels=top_labels,\n",
    "        labels=(pred_index,),\n",
    "        hide_color=hide_color,\n",
    "        num_features=num_lime_features,\n",
    "        num_samples=num_samples,\n",
    "        random_seed = 0) # for reproductibility\n",
    "    \n",
    "    return explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Display the explanation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.segmentation import mark_boundaries\n",
    "\n",
    "\n",
    "def explain_with_lime(img_path, \n",
    "                      top_labels, hide_color, num_lime_features, num_samples, # Explanation parameters\n",
    "                      positive_only, negative_only, num_superpixels, hide_rest# Rendering parameters\n",
    "                      ):\n",
    "\n",
    "    img_array = get_img_array(img_path, size=img_size)\n",
    "    \n",
    "    _, pred_index, labels = make_prediction(img_array)\n",
    "    \n",
    "    print(\"Top-3 predicted classes : \")\n",
    "    for l in labels:\n",
    "        print(\"\\t\"+l[0]+\": \"+str(l[1]))\n",
    "\n",
    "\n",
    "    #Display the image\n",
    "    plt.imshow(img_array[0] /2 +0.5) #for rendering because of preprocessin of Xception\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "    explanation = get_lime_explanation(img_array[0],\n",
    "                                        pred_index, top_labels, hide_color, num_lime_features, num_samples)\n",
    "\n",
    "    temp, mask = explanation.get_image_and_mask(pred_index, \n",
    "                                                positive_only=positive_only, negative_only=negative_only, num_features=num_superpixels, hide_rest=hide_rest)\n",
    "\n",
    "    plt.imshow(mark_boundaries(temp / 2 + 0.5, mask))\n",
    "    plt.axis('off')  \n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main code to execute\n",
    "\n",
    "Bellow you can find the code to choose the image file, set all parameters considered here and compute the explanation of the model prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the image whose prediction is to be explained \n",
    "img_path = \"./data/African_elephant/ILSVRC2012_val_00048781.JPEG\"\n",
    "\n",
    "# Explanation parameters\n",
    "top_labels        = 1 # Use top-k labels or not\n",
    "hide_color        = [0,0,0] # RGB color or None (average color of superpixels is used) used to generate neighboring samples\n",
    "num_lime_features = 100000 # size in number of groups of features of an explanation\n",
    "num_samples       = 5000 # number of perturbated samples to generate\n",
    "\n",
    "# Rendering parameters\n",
    "positive_only   = True # display only features having a positive impact on the prediction\n",
    "negative_only   = False # display only features having a negative impact on the prediction\n",
    "num_superpixels = 15 # number of superpixels to display\n",
    "hide_rest       = True # hide the rest of the picture or not\n",
    "\n",
    "explain_with_lime(img_path, \n",
    "                  top_labels, hide_color, num_lime_features, num_samples,\n",
    "                  positive_only, negative_only, num_superpixels, hide_rest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "### Question 1\n",
    "As you can see in the previous cell, many parameters have to be set manually according to the model and data. Try to identify a right combination of parameters to explain the prediction of the given image (here an african elephant). Try different values for the parameters and show which combination of the values produces the better explanation visually.\n",
    "\n",
    "### Question 2\n",
    "Now consider another image of african elephan (see \"./data/African_elephant/\"). Is your parameter setting stil appropriate? How does the output look different now?\n",
    "\n",
    "### Question 3\n",
    "We now consider images from another class to assess whether the identified setting is appropriate for another class. You can find a black bear images here: \"./data/black_bear/\"\n",
    "\n",
    "What can you conclude? \n",
    "\n",
    "### Question 4\n",
    "Here, we want to answer the following question: If we change the model, would the parameter setting still be appropriate? In other words, is the parameter setting more related to the data and tasks than it is to the model architecture?\n",
    "\n",
    "1. Below, you can find the source code for loading a pre-trained Resnet model. Try to explain its prediction with LIME and to identify a good parameter setting.\n",
    "\n",
    "2. What can you conclude?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "model_builder = tf.keras.applications.resnet_v2.ResNet50V2\n",
    "preprocess_input = tf.keras.applications.resnet_v2.preprocess_input\n",
    "decode_predictions = tf.keras.applications.resnet_v2.decode_predictions\n",
    "model = model_builder(weights=\"imagenet\", classifier_activation=\"softmax\")\n",
    "\n",
    "# expected input size for ResNet50\n",
    "img_size = (224,224)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML_ENV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
